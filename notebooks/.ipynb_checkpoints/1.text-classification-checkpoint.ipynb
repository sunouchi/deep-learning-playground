{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他人はみんなクソだ。特に女は最悪だ。それが柏木大吾が十七年の人生で学び取ったことだった。大吾が物心ついた時、父親の姿は既になかった。死んではいないと思われる。母親に逃げられたか、そもそも行きずりの相手で父親が誰だかわからないのか、そのどちらかだと大吾は思っている。大吾の母親は一言で言えばクズだった。幼い子供を家に残して一日中遊びほうけるような。そして、たまにいるかと思えば、暴力を振るうような。そんな母親だった。\n"
     ]
    }
   ],
   "source": [
    "# テキストデータの内容を読み込む処理\n",
    "def load_data(path, sent_size=None):\n",
    "    f = open(path)\n",
    "    raw = ''\n",
    "    for i, line in enumerate(f):\n",
    "        # 0行目は作品名、1行目はURLなので、2行目以降を取得する\n",
    "        if i >= 2:\n",
    "            raw += line\n",
    "    raw = \"\".join(raw.split())\n",
    "    # 指定された数だけ文を取り出す\n",
    "    delimitor = '。'\n",
    "    sentences = raw.split(delimitor)\n",
    "    if sent_size:\n",
    "        try:\n",
    "            sent_size.__index__\n",
    "        except AttributeError as err:\n",
    "            raise TypeError('An integer is required.') from err\n",
    "        sentences = sentences[:sent_size]\n",
    "    return delimitor.join(sentences) + delimitor\n",
    "\n",
    "# 入力データの読み込み\n",
    "import os\n",
    "input_dir = '../data/novel'\n",
    "# デバッグ利便性のためにサイズを小さくする\n",
    "sent_size = 10\n",
    "horror_raw = load_data(os.path.join(input_dir, 'horror.txt'), sent_size)\n",
    "isekai_raw = load_data(os.path.join(input_dir, 'isekai.txt'), sent_size)\n",
    "love_raw   = load_data(os.path.join(input_dir, 'love.txt'), sent_size)\n",
    "\n",
    "print(horror_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['他人', 'は', 'みんな', 'クソ', 'だ', '。', '特に', '女', 'は', '最悪', 'だ', '。', 'それ', 'が', '柏木', '大', '吾', 'が', '十', '七', '年', 'の', '人生', 'で', '学び取っ', 'た', 'こと', 'だっ', 'た', '。', '大', '吾', 'が', '物心', 'つい', 'た', '時', '、', '父親', 'の', '姿', 'は', '既に', 'なかっ', 'た', '。', '死ん', 'で', 'は', 'い', 'ない', 'と', '思わ', 'れる', '。', '母親', 'に', '逃げ', 'られ', 'た', 'か', '、', 'そもそも', '行きずり', 'の', '相手', 'で', '父親', 'が', '誰', 'だ', 'か', 'わから', 'ない', 'の', 'か', '、', 'その', 'どちら', 'か', 'だ', 'と', '大', '吾', 'は', '思っ', 'て', 'いる', '。', '大', '吾', 'の', '母親', 'は', '一言', 'で', '言え', 'ば', 'クズ', 'だっ', 'た', '。', '幼い', '子供', 'を', '家', 'に', '残し', 'て', '一', '日', '中', '遊び', 'ほうける', 'よう', 'な', '。', 'そして', '、', 'たま', 'に', 'いる', 'か', 'と', '思え', 'ば', '、', '暴力', 'を', '振るう', 'よう', 'な', '。', 'そんな', '母親', 'だっ', 'た', '。']\n"
     ]
    }
   ],
   "source": [
    "# 読み込んだライトノベルのデータを、形態素に分割してトークナイズする\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "def tokenize(text):\n",
    "    # 抽出した素性を配列に変換する\n",
    "    tags = tagger.parse(horror_raw).split('\\n')\n",
    "    # 素性から原型のみを配列にする\n",
    "    tokens = []\n",
    "    for t in tags:\n",
    "        token = t.split('\\t')[0]\n",
    "        if token not in ('', 'EOS'):\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "horror = tokenize(horror_raw)\n",
    "print(horror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 6-6: Embedding層で使用する IMDb データを読み込む\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# データを整数のリストとして読み込む\n",
    "(x_train, y_train), (x_test, y_test) = \\\n",
    "    imdb.load_data(num_words=max_features)\n",
    "word_index = imdb.get_word_index()\n",
    "reversed_word_index = {v:k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'the', 'disaster', 'that', 'was', 'the', \"80's\", 'and', 'have', 'a', 'good', 'old', 'laugh', 'at', 'how', 'bad', 'everything', 'was', 'back', 'then']\n"
     ]
    }
   ],
   "source": [
    "# 特徴量として考慮する単語の数\n",
    "max_features = 10000\n",
    "\n",
    "# max_features個の最も出現頻度の高い単語のうち、\n",
    "# 個の数の単語を残してテキストをカット\n",
    "max_len = 20\n",
    "\n",
    "# print(word_index)\n",
    "print([reversed_word_index.get(x-3, '?') for x in x_train[1]])\n",
    "    \n",
    "\n",
    "# 整数のリストを形状が (samples, max_len) の整数型の2次元テンソルに変換\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
